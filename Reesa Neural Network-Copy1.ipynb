{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural netowrk funcs\n",
    "\n",
    "Layers: an Integer value representing the total number of hidden layers in the network\n",
    "(input and output layers are extra)\n",
    "\n",
    "● Nodes: an integer array of size [0,..,Layers+1] containing the dimensions of the neural\n",
    "network. Nodes[0] shall represent the input size (typically, 50), Nodes[Layers+1]\n",
    "\n",
    "shall represent the number of output nodes (typically, 1). All other values Nodes[i]\n",
    "represent the number of nodes in hidden layer i.\n",
    "● NNodes: a possible alternative to the Nodes parameter for situations where you want\n",
    "each hidden layer of the neural network to be of the same size. In this case, the size of\n",
    "the output layer is assumed to be 1, and the size of the input layer can be inferred from\n",
    "the dataset.\n",
    "● Activations: an array of size [0,..,Layers+1] (for the sake of compatibility) in which\n",
    "Activations[0] and Activations[Layers+1] are not used, while all other\n",
    "Activations[i] values are labels indicating the activation function used in layer i.\n",
    "This allows you to build neural networks with different activation functions in each layer.\n",
    "● ActivationFn: a possible alternative to Activations when all hidden layers of your neural\n",
    "network use the same activation function.\n",
    "\n",
    "params: layers: # of hidden layers, \n",
    "\tnodes: if int: number of nodes for each hidden layer (when creating hidden layer, add 1 extraa node for bias term)\n",
    "\t       if arr: number elems in arr should be same as layers, each num will be number of nodes for the hidden layer\n",
    "\tactivation: apply to all hidden layers\n",
    "\tlearning rate:\n",
    "\tbatch size:\n",
    "\n",
    "\n",
    "when initializing, need to generate random weights for all the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x: num_obs x num_features\n",
    "\n",
    "w1+b: num_features(x) + 1 x width + 1 (of h1)\n",
    "\n",
    "h1 = num_obs x width+1(of h2)\n",
    "\n",
    "w2+b: width+1(of h1) x width+1 (of h2)\n",
    "*****\n",
    "\n",
    "h2 = num_obs x width+1(of h2) (of h2)\n",
    "\n",
    "w3+b = width+1(of h2) x 1(we only want it output a single value per observation)\n",
    "\n",
    "\n",
    "y = num_obs x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, nodes, activation, loss, learning_rate, batch_size):\n",
    "        \n",
    "        # number of hidden layers we are creating\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # number of nodes at each layer\n",
    "        self.nodes = self._create_nodes(nodes,layers)\n",
    "        \n",
    "        # activation function\n",
    "        self.sigma = activation\n",
    "                \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # learning rate for stochastic gradient descent\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # batch size for stochastic gradient descent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # after fit, will contain a dictionary of weights for the layers\n",
    "        self.weight_dict = None\n",
    "        \n",
    "        # store values for backwards propagation\n",
    "        self.stored = {}\n",
    "\n",
    "        \n",
    "    def _create_nodes(self, nodes, layers):\n",
    "    \n",
    "        if isinstance(nodes, int):\n",
    "            nodes = nodes+1\n",
    "            return [nodes]*(layers)\n",
    "        \n",
    "        else:\n",
    "            return [i+1 for i in nodes]\n",
    "        \n",
    "    def _create_weights(self, num_features):\n",
    "        \n",
    "        # for 3 hidden layers\n",
    "        \n",
    "        #0 between input layer and hidden layer 1: \n",
    "        #1 hidden layer 1 to hidden layer 2: width x width\n",
    "        #2 hidden layer 2 to hidden layer 3: width x width\n",
    "        #3 hidden layer 3 to y :width x 1\n",
    "        \n",
    "        #so self.layers[0] will always be numfeatures x width(of first hidden layer)\n",
    "        # self.layers[num_layers] = widthx1\n",
    "        \n",
    "        self.weight_dict = {\n",
    "            0: np.random.rand(num_features, self.nodes[0])\n",
    "        }\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            self.weight_dict[i] = np.random.rand(self.nodes[i-1], self.nodes[i])\n",
    "            \n",
    "        self.weight_dict[self.num_layers] = np.random.rand(self.nodes[self.num_layers-1], 1)\n",
    "        \n",
    "        \n",
    "    def _forward_propagation(self, X, y):\n",
    "        # need to figure out what weights to save and how to backprop\n",
    "        \n",
    "        sigma_h_x = X\n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i]\n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "            self.stored[i] = [h_x, sigma_h_x]\n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[self.num_layers]\n",
    "        self.stored[self.num_layers] = [z_sigma_h_x]\n",
    "        \n",
    "        return np.sum(self.loss(z_sigma_h_x, y))/self.batch_size #summation of num_obs x 1 array divide by batchsize\n",
    "        \"\"\"\n",
    "        2 hidden layer example\n",
    "        weight 0: [h1(x), sigma(h1(x))] #X to h1\n",
    "        weight 1: [h2(sigma(h1(x))), sigma(h2(sigma(h1(x))))] #h1 to h2 [bxn], [bxn]\n",
    "        weight 2: [z(sigma(h2(sigma(h1(x)))))] #h2 to y [nx1]\n",
    "        \"\"\"\n",
    "    def _backward_propagation(self, X,y):\n",
    "\n",
    "        expected = self.stored[self.num_layers][0] #z(sigma(h2(sigma(h1(x)))))\n",
    "        \n",
    "         # dL/dz(z(sigma(h2(sigma(h1(x))))))\n",
    "        J = self.loss(expected, y, derivative = True)\n",
    "        old_weights = self.weight_dict[self.num_layers] \n",
    "        \n",
    "        #updating the weights\n",
    "        #dz/dw2(sigma(h2(sigma(h1(x)))))\n",
    "        self.weight_dict[num_layers] = self.weight_dict[num_layers] - self.lr*(self.stored[num_layers-1][1].T @ J)\n",
    "        #dz/dsigma(sigma(h2(sigma(h1(x)))))\n",
    "        J = J @ old_weights.T \n",
    "        \n",
    "        ################################################################################\n",
    "        \n",
    "        #dsigma/dh2(h2(sigma(h1(x))))) # i dont understand\n",
    "        J = self.sigma(J, derivative = True)\n",
    "        # J = self.sigma(self.stored[num_layers-1][0], derivative)\n",
    "        \n",
    "        #updating the weights\n",
    "        #dh2/w1\n",
    "        #dh2/sigma(sigma(h2(sigma(h1(x)))))\n",
    "        old_weights = self.weight_dict[self.num_layers-1] \n",
    "        self.weight_dict[num_layers-1] = self.weight_dict[num_layers-1] -self.lr*(self.stored[num_layers-2][1].T @ J)\n",
    "        \n",
    "        #updating the weights\n",
    "        #dh2/dsigma(sigma(h2(sigma(h1(x)))))\n",
    "        J = J @ old_weights.T\n",
    "        \n",
    "        # network: X -w0-sigma-> h1 -w1-sigma-> h2 -w2-> y\n",
    "        \n",
    "        #1. J = loss_deriv(z(sigma(h2(sigma(h1(x))))))\n",
    "        #2. Update weight w2 = w2 - learning_rate*(sigma(h2(sigma(h1(x)))) @ J)\n",
    "        \n",
    "        #3  Dense layer derivative on z AKAJ =J @ oldweights.T\n",
    "        #4. Activation Layer derivative of J: J = J* sigmad_deriv(h2(sigma(h1(x))))\n",
    "        #5. Update weight w1 = w1 - learning_rate*(sigma(h1(x))) @ J)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # dense layer derivative on z AKA J @ oldweights.T\n",
    "         #4. Activation Layer derivative of J: J = J* sigmad_deriv((h1(x))))\n",
    "            \n",
    "            #5. Update weight w0 = w0 - learning_rate*(x)) @ J)\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers,0,-1):\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            old_weights = self.weight_dict[i]\n",
    "            self.weight_dict[i] = self.weight_dict[i] -self.lr*(self.stored[i-1][1].T @ J) #dz/dw | dh/dw\n",
    "            print(self.stored[i-1][1].T.shape)\n",
    "            print(J.shape)\n",
    "            J =  self.stored[i-1][1].T @ J #dz/dsigma  | dh/dsigma\n",
    "            print(\"\\n\")\n",
    "            print(J.shape)\n",
    "            print(self.sigma(self.stored[i-1][0], derivative = True).shape)\n",
    "            J =  J @ self.sigma(self.stored[i-1][0], derivative = True) #dsigma/dh\n",
    "            \n",
    "\n",
    "        #final weight update\n",
    "        self.weight_dict[0] = X.T @ J #dh1/dw0(x)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "  dZ1 = np.multiply(dA1, A1 * (1 - A1)) \n",
    "    def _stochastic_gradient_descent(self, X, y, num_iterations, print_iter):\n",
    "        # how to integrate forward and back vias sgd\n",
    "        \n",
    "        avg_err_arr = [np.nan]*num_iterations\n",
    "        \n",
    "        indices = np.random.choice(X.shape[0], self.batch_size, replace=False)\n",
    "        x_batch = X[indices]\n",
    "        y_batch =y[indices]\n",
    "        \n",
    "        avg_err_arr[0] = self._forward_propagation(batch,y_batch)\n",
    "        \n",
    "        for i in range(1,num_iterations):\n",
    "\n",
    "            self._backward_propagation(x_batch,y_batch)\n",
    "            indices = np.random.choice(X.shape[0], self.batch_size, replace=False)\n",
    "            x_batch = X[indices]\n",
    "            y_batch =y[indices]\n",
    "        \n",
    "            avg_err_arr[i] = self._forward_propagation(batch,y_batch)\n",
    "            \n",
    "        \n",
    "        return avg_err_arr\n",
    "        \n",
    "    \n",
    "    def fit(self, X,y, num_iterations = 1000, print_iter = False):\n",
    "        \n",
    "        # add intercept to X\n",
    "        X = np.hstack([np.ones(len(x))[:, np.newaxis], x])  \n",
    "        y = np.array([y]).T # makes it easier to calculate loss\n",
    "        \n",
    "        # generate the random weights for every layer in neural network\n",
    "        self._create_weights(X.shape[1])\n",
    "        \n",
    "        self._stochastic_gradient_descent(X,y,num_iterations, print_iter)\n",
    "        \n",
    "        #self._forward_propagation(X,y)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # forward propagation without saved vals for speed\n",
    "    def predict(self, X):\n",
    "        sigma_h_x = X\n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i]\n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[num_layers]\n",
    "        return z_sigma_h_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2_loss(x,y, derivative = False):\n",
    "    # assumes y is nx1 vector (n rows, 1 col)\n",
    "    if derivative:\n",
    "        return x-y\n",
    "    else:\n",
    "        return 0.5*((x-y)**2)\n",
    "        \n",
    "def ReLU(X, derivative = False):\n",
    "    if derivative:\n",
    "        # faster than np.where\n",
    "        np.greater(X, 0).astype(int)\n",
    "    return np.maximum(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 3\n",
    "nodes = 1\n",
    "activation = ReLU\n",
    "loss = L2_loss\n",
    "learning_rate = 0.001\n",
    "batch_size = 2\n",
    "\n",
    "\n",
    "test_nn = NeuralNetwork(layers, nodes, activation, loss, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    [1,2,1],\n",
    "    [3,5,5],\n",
    "    [3,1,2]\n",
    "    \n",
    "])\n",
    "\n",
    "y = np.array([[5,14,7]]).T\n",
    "\n",
    "\n",
    "test_nn._create_weights(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2159043 ],\n",
       "       [0.96872786]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.weight_dict[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.099576927450684"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn._forward_propagation( X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44527635, 2.01644947],\n",
       "       [4.63699299, 6.05289848],\n",
       "       [2.20976458, 2.6436362 ]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.stored[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44527635, 2.01644947],\n",
       "       [4.63699299, 6.05289848],\n",
       "       [2.20976458, 2.6436362 ]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.stored[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.869055  , 2.56440005],\n",
       "       [2.70716886, 7.95124233],\n",
       "       [1.24322286, 3.62941617]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn.stored[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(3, 1)\n",
      "\n",
      "\n",
      "(2, 1)\n",
      "(3, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-5b2b1ffb244a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-185-16087546bc04>\u001b[0m in \u001b[0;36m_backward_propagation\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mJ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderivative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mJ\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mJ\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstored\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mderivative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#dsigma/dh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
     ]
    }
   ],
   "source": [
    "test_nn._backward_propagation(X,y)\n",
    "test_nn._forward_propagation(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n#         Jw1 = self.stored[self.num_layers-2][1].T @ J #dh2/dw1(sigma(h1(x)))\\n#         J =  J @ self.sigma(self.stored[self.num_layers-2][1], derivative = True) #dh2/dsigma(sigma(h1(x)))\\n#         J =  J @ self.stored[self.num_layers-2][0].T  #dsigma/dh1(h1(x))\\n        \\n\\n'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "#         Jw1 = self.stored[self.num_layers-2][1].T @ J #dh2/dw1(sigma(h1(x)))\n",
    "#         J =  J @ self.sigma(self.stored[self.num_layers-2][1], derivative = True) #dh2/dsigma(sigma(h1(x)))\n",
    "#         J =  J @ self.stored[self.num_layers-2][0].T  #dsigma/dh1(h1(x))\n",
    "        \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
