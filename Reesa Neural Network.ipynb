{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code\n",
    "\n",
    "The only functions of importance when being used are `fit` and `predict`, which can be used like almost any model in \n",
    "`scikit-learn`. \n",
    "\n",
    "Everything else is a helper method, and `Layer` is a helper class. I have included comments on how this code works. Please delete the sections marked delete after you understand them. I'll modify this file and my notes to you guys shortly before submission as well.\n",
    "\n",
    "Things to note:\n",
    "- `nodes` can be either an array or a single number. If you give an array, it must be the same length as `layers`\n",
    "- `activation` can only be a function. This implementation does not accept multiple activation functions\n",
    "- `loss` must return an nx1 array, and must assume its inputs are also nx1\n",
    "- Both `activation` and `loss` must have the parameter `derivative`, where when set to true, returns the derivative given the inputs it receives\n",
    "- The print_iter argument in fit allows to see what number error we are appending to the error array as an estimation of time left in a non_cluttering manner.\n",
    "\n",
    "- Please look at my example down below if you want to write a custom function, but I already implemented the functions that Professor Ventura said was best for this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # essentially a container to hold stored values across layers in forward propogation\n",
    "    def __init__(self, h_x = None, sigma_h_x = None):\n",
    "        self.h_x = h_x\n",
    "        self.sigma_h_x = sigma_h_x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, nodes, activation, loss, learning_rate, batch_size):\n",
    "        \n",
    "        # number of hidden layers we are creating\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # number of nodes at each layer\n",
    "        self.nodes = self._create_nodes(nodes,layers)\n",
    "        \n",
    "        # activation function\n",
    "        self.sigma = activation\n",
    "                \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # learning rate for stochastic gradient descent\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # batch size for stochastic gradient descent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # after fit, will contain a dictionary of weights for the layers\n",
    "        self.weight_dict = None\n",
    "        \n",
    "        # store values for backwards propagation\n",
    "        self.stored = None\n",
    "\n",
    "        \n",
    "    def _create_nodes(self, nodes, layers):\n",
    "        # if given a number, creates an array with layers amount of nodes\n",
    "        # otherwise returns the array of nodes\n",
    "        # in both cases increments by 1 to account for bias\n",
    "        if isinstance(nodes, int):\n",
    "            nodes = nodes+1\n",
    "            return [nodes]*(layers)\n",
    "        \n",
    "        else:\n",
    "            if len(nodes) != self.num_layers:\n",
    "                raise Exception(\"Node array length does not match number of layers\")\n",
    "            return [i+1 for i in nodes]\n",
    "        \n",
    "    def _create_weights(self, num_features):\n",
    "        # generates a dictionary of initial weights in fit\n",
    "        # delete below if you understand this\n",
    "        # first layer is num_features x num nodes[0]\n",
    "        # middle layers are nodes[i-1] x nodes[i]\n",
    "        # last layers is num_nodes[num_layers-1](represents last layer) x1 (output)\n",
    "        \n",
    "        self.weight_dict = {\n",
    "            0: np.random.rand(num_features, self.nodes[0])\n",
    "        }\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            self.weight_dict[i] = np.random.rand(self.nodes[i-1], self.nodes[i])\n",
    "            \n",
    "        self.weight_dict[self.num_layers] = np.random.rand(self.nodes[self.num_layers-1], 1)\n",
    "\n",
    "    def _forward_propagation(self, X, y):\n",
    "        # performs forward_propagation and stores appropriate values\n",
    "        \n",
    "        sigma_h_x = X\n",
    "        self.stored = {}\n",
    "        self.stored[-1] = Layer(None, sigma_h_x)\n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i] \n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "            self.stored[i] = Layer(h_x, sigma_h_x)\n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[self.num_layers]\n",
    "        self.stored[self.num_layers] = Layer(z_sigma_h_x)\n",
    "        \n",
    "        # average loss\n",
    "        return np.sum(self.loss(z_sigma_h_x, y))/self.batch_size \n",
    "    \n",
    "        \"\"\"\n",
    "        2 hidden layer example DELETE IF YOU UNDERSTAND\n",
    "        dictkey in self.stored: [layer.h_x, layer.sigma_h_x]\n",
    "        -1:[None, x]\n",
    "        0: [h1(x), sigma(h1(x))] #X to h1\n",
    "        1: [h2(sigma(h1(x))), sigma(h2(sigma(h1(x))))] #h1 to h2 [bxn], [bxn]\n",
    "        2: [z(sigma(h2(sigma(h1(x)))))] #h2 to y [nx1]\n",
    "        \"\"\"\n",
    "    def _backward_propagation(self, X,y):\n",
    "        # updates the weights based on the stored values from forward propagation\n",
    "        \n",
    "        num_layers = self.num_layers\n",
    "        expected = self.stored[self.num_layers].h_x #z(sigma(h2(sigma(h1(x)))))\n",
    "        \n",
    "        # dL/dz(z(sigma(h2(sigma(h1(x))))))\n",
    "        J = self.loss(expected, y, derivative = True)\n",
    "        \n",
    "        old_weights = self.weight_dict[num_layers] \n",
    "        self.weight_dict[num_layers] = self.weight_dict[num_layers] - self.lr*(self.stored[num_layers-1].sigma_h_x.T @ J)\n",
    "        # dz/dsigma(sigma(h2(sigma(h1(x)))))\n",
    "        \n",
    "        for i in range(self.num_layers-1,-1,-1):\n",
    "            \n",
    "            J = J * self.sigma(self.stored[i].h_x, derivative = True) # activation layer derivative\n",
    "            \n",
    "            old_weights = self.weight_dict[i] \n",
    "            self.weight_dict[i] = self.weight_dict[i] - self.lr*(self.stored[i-1].sigma_h_x.T @ J) # weight update\n",
    "            \n",
    "            J = J @ old_weights.T  # dense layer derivative\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    def _stochastic_gradient_descent(self, X, y, num_epochs, print_iter):\n",
    "        \n",
    "        num_batches = math.ceil(X.shape[0]/self.batch_size)\n",
    "        avg_err_arr = [np.nan]*(num_epochs * num_batches)\n",
    "        \n",
    "        err_count = 0\n",
    "        \n",
    "        for i in range(0, num_epochs):\n",
    "            \n",
    "            indices = np.random.choice(X.shape[0], X.shape[0], replace=False)\n",
    "            x_batch = X[indices]\n",
    "            y_batch =y[indices]\n",
    "            \n",
    "            old_start = 0\n",
    "            new_start = 0\n",
    "            new_end = self.batch_size\n",
    "            \n",
    "            avg_err_arr[err_count] = self._forward_propagation(x_batch[new_start:new_end],y_batch[new_start:new_end])\n",
    "            err_count+=1\n",
    "            \n",
    "            for j in range(1,num_batches):\n",
    "                old_start = new_start\n",
    "                new_start = old_start + self.batch_size\n",
    "                new_end = new_start+self.batch_size\n",
    "                \n",
    "                self._backward_propagation(x_batch[old_start:new_start], y_batch[old_start:new_start])\n",
    "                avg_err_arr[err_count] = self._forward_propagation(x_batch[new_start:new_end],y_batch[new_start:new_end])\n",
    "                err_count+=1\n",
    "                \n",
    "                if print_iter:\n",
    "                    sys.stdout.write(\"\\r\" +  str(err_count))\n",
    "                    sys.stdout.flush()\n",
    "                    \n",
    "            self._backward_propagation(x_batch[new_start:new_end],y_batch[new_start:new_end])\n",
    "\n",
    "        return avg_err_arr\n",
    "        \n",
    "    \n",
    "    def fit(self, X,y, num_epochs = 10, print_iter = False):\n",
    "        # X is either a numpy array or a dataframe without the target and intercept\n",
    "        # y is either a pandas series or a 1d (potentially numpy) array\n",
    "        # returns a list of the average loss for each iteration\n",
    "        # if print iter is true, prints the current iteration (will be written on a single line, not multiple)\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "            \n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_numpy()\n",
    "        \n",
    "        # add intercept to X\n",
    "        X = np.hstack([np.ones(len(X))[:, np.newaxis], X])  \n",
    "        y = np.array([y]).T # makes it easier to calculate loss\n",
    "        \n",
    "        # generate the random weights for every layer in neural network\n",
    "        self._create_weights(X.shape[1])\n",
    "        \n",
    "        \n",
    "        return self._stochastic_gradient_descent(X,y,num_epochs, print_iter)\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # does a single forward propagation without saving any values\n",
    "        # returns a 1xn array of predictions\n",
    "        sigma_h_x = np.hstack([np.ones(len(X))[:, np.newaxis], X])  \n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i]\n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[self.num_layers]\n",
    "        return z_sigma_h_x.T[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Loss and Activation Function\n",
    "\n",
    "You can use it like so:\n",
    "\n",
    "```\n",
    "test_nn = NeuralNetwork(layers, nodes, activation=ReLU, loss=L2_loss, learning_rate, batch_size)\n",
    "```\n",
    "Just simply enter the name of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and activation function to use in \n",
    "\n",
    "def L2_loss(x,y, derivative = False):\n",
    "    # assumes y is nx1 vector (n rows, 1 col)\n",
    "    # returns an nx1 matrix\n",
    "    if derivative:\n",
    "        return x-y\n",
    "    else:\n",
    "        return 0.5*((x-y)**2)\n",
    "        \n",
    "def ReLU(X, derivative = False):\n",
    "    if derivative:\n",
    "        # faster than np.where\n",
    "        return np.greater(X, 0).astype(int)\n",
    "    return np.maximum(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
       "0               3.5                1.4               0.2     5.1\n",
       "1               3.0                1.4               0.2     4.9\n",
       "2               3.2                1.3               0.2     4.7\n",
       "3               3.1                1.5               0.2     4.6\n",
       "4               3.6                1.4               0.2     5.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "data = sklearn.datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"target\"] = df[\"sepal length (cm)\"]\n",
    "df = df.drop(\"sepal length (cm)\", axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.8, 5.843333333333335, 0.8280661279778629)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.median(),df.target.mean(),df.target.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.3, 7.9)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.min(),df.target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100"
     ]
    }
   ],
   "source": [
    "layers = 1\n",
    "nodes = 3\n",
    "activation = ReLU\n",
    "loss = L2_loss\n",
    "learning_rate = 0.0001\n",
    "batch_size = 30\n",
    "num_epochs = 20\n",
    "num_runs = math.ceil(df.shape[0]/batch_size)* num_epochs\n",
    "\n",
    "test_nn = NeuralNetwork(layers, nodes, activation, loss, learning_rate, batch_size)\n",
    "\n",
    "errs = test_nn.fit(df.drop(\"target\",axis=1),df[\"target\"], num_epochs = num_epochs, print_iter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.756365957248606,\n",
       " 1.191017365812171,\n",
       " 0.5448939006172007,\n",
       " 0.5211627287656669,\n",
       " 0.42621236209131835,\n",
       " 0.35652758504129944,\n",
       " 0.46350150743717233,\n",
       " 0.48844808454312866,\n",
       " 0.40333779633464545,\n",
       " 0.35988591225326005,\n",
       " 0.3113607827717485,\n",
       " 0.48621340273795044,\n",
       " 0.3268675296423785,\n",
       " 0.3487383840179292,\n",
       " 0.4680099128308464,\n",
       " 0.42718122372154815,\n",
       " 0.4193865347938898,\n",
       " 0.3144624284870274,\n",
       " 0.30305799817179735,\n",
       " 0.37736617908846565,\n",
       " 0.37939714701132965,\n",
       " 0.43560750923601854,\n",
       " 0.4021159562608548,\n",
       " 0.30776333490950003,\n",
       " 0.23915436389897918,\n",
       " 0.3808888928503786,\n",
       " 0.3819301016955274,\n",
       " 0.23446792641201303,\n",
       " 0.38894967812130227,\n",
       " 0.28384294121990966,\n",
       " 0.232681598988335,\n",
       " 0.3055614202595909,\n",
       " 0.31472874912095083,\n",
       " 0.33164234498474093,\n",
       " 0.39002372024854126,\n",
       " 0.3002937499450464,\n",
       " 0.2556281144820993,\n",
       " 0.37752643450546,\n",
       " 0.2638819633242001,\n",
       " 0.3156226320155809,\n",
       " 0.3188814699316419,\n",
       " 0.22493012116173464,\n",
       " 0.3451003969802982,\n",
       " 0.3183063695329498,\n",
       " 0.24578081655251102,\n",
       " 0.2946632687303092,\n",
       " 0.23821756317219553,\n",
       " 0.26988498115093124,\n",
       " 0.2996257849677106,\n",
       " 0.26740609209799276,\n",
       " 0.3429304696495425,\n",
       " 0.22824638521110954,\n",
       " 0.22331733721386796,\n",
       " 0.2539561949794338,\n",
       " 0.2646175841352758,\n",
       " 0.2859247625391302,\n",
       " 0.29085305133779255,\n",
       " 0.2422357457738831,\n",
       " 0.21946962292247232,\n",
       " 0.2310301879337862,\n",
       " 0.22015369111177385,\n",
       " 0.19129650856860542,\n",
       " 0.28105868779616533,\n",
       " 0.23145241012661816,\n",
       " 0.2839393440793291,\n",
       " 0.2488457668727551,\n",
       " 0.21226027249712967,\n",
       " 0.2643966227247783,\n",
       " 0.24594605325734814,\n",
       " 0.1817521575384197,\n",
       " 0.23732169422642882,\n",
       " 0.21435893328787542,\n",
       " 0.2950124204365757,\n",
       " 0.230245087692792,\n",
       " 0.14278824810951476,\n",
       " 0.22729184391659304,\n",
       " 0.17728950329934634,\n",
       " 0.24510363640812913,\n",
       " 0.22113975346256487,\n",
       " 0.1984683771159195,\n",
       " 0.20213949793507807,\n",
       " 0.20619727921583758,\n",
       " 0.2379360579138272,\n",
       " 0.2091793252734167,\n",
       " 0.17978330543136548,\n",
       " 0.20656752667199776,\n",
       " 0.18013387773781175,\n",
       " 0.20872649244133024,\n",
       " 0.17169700019556272,\n",
       " 0.23650044871202067,\n",
       " 0.20021910183777802,\n",
       " 0.21328022208361433,\n",
       " 0.16574320134683598,\n",
       " 0.25107347031849,\n",
       " 0.12947558606487133,\n",
       " 0.17176656043663596,\n",
       " 0.20184615949680731,\n",
       " 0.1822426176352524,\n",
       " 0.2009323991282442,\n",
       " 0.17335261458984738]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3494281397937803"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_size = 50\n",
    "samp = df.sample(n=samp_size)\n",
    "\n",
    "\n",
    "preds = test_nn.predict(samp.drop(\"target\",axis=1))\n",
    "mse = ((preds -samp.target)**2).sum()/samp_size\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2adade02508>]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "ax.plot([i for i in range(0,num_runs)], errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unstable looking result above is normal of stochastic gradient descent, especially if your batch size is small in relation to the overall dataset, as it is an approximation. \n",
    "\n",
    "So as long as the graph shows a general trend downwards, it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
