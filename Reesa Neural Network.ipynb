{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Code\n",
    "\n",
    "The only functions of importance when being used are `fit` and `predict`, which can be used like almost any model in \n",
    "`scikit-learn`. \n",
    "\n",
    "Everything else is a helper method, and `Layer` is a helper class. I have included comments on how this code works. Please delete the sections marked delete after you understand them. I'll modify this file and my notes to you guys shortly before submission as well.\n",
    "\n",
    "Things to note:\n",
    "- `nodes` can be either an array or a single number. If you give an array, it must be the same length as `layers`\n",
    "- `activation` can only be a function. This implementation does not accept multiple activation functions\n",
    "- `loss` must return an nx1 array, and must assume its inputs are also nx1\n",
    "- Both `activation` and `loss` must have the parameter `derivative`, where when set to true, returns the derivative given the inputs it receives\n",
    "- Please look at my example down below if you want to write a custom function, but I already implemented the functions that Professor Ventura said was best for this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # essentially a container to hold stored values across layers in forward propogation\n",
    "    def __init__(self, h_x = None, sigma_h_x = None):\n",
    "        self.h_x = h_x\n",
    "        self.sigma_h_x = sigma_h_x\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, layers, nodes, activation, loss, learning_rate, batch_size):\n",
    "        \n",
    "        # number of hidden layers we are creating\n",
    "        self.num_layers = layers\n",
    "        \n",
    "        # number of nodes at each layer\n",
    "        self.nodes = self._create_nodes(nodes,layers)\n",
    "        \n",
    "        # activation function\n",
    "        self.sigma = activation\n",
    "                \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # learning rate for stochastic gradient descent\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # batch size for stochastic gradient descent\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # after fit, will contain a dictionary of weights for the layers\n",
    "        self.weight_dict = None\n",
    "        \n",
    "        # store values for backwards propagation\n",
    "        self.stored = None\n",
    "\n",
    "        \n",
    "    def _create_nodes(self, nodes, layers):\n",
    "        # if given a number, creates an array with layers amount of nodes\n",
    "        # otherwise returns the array of nodes\n",
    "        # in both cases increments by 1 to account for bias\n",
    "        if isinstance(nodes, int):\n",
    "            nodes = nodes+1\n",
    "            return [nodes]*(layers)\n",
    "        \n",
    "        else:\n",
    "            if len(nodes) != self.num_layers:\n",
    "                raise Exception(\"Node array length does not match number of layers\")\n",
    "            return [i+1 for i in nodes]\n",
    "        \n",
    "    def _create_weights(self, num_features):\n",
    "        # generates a dictionary of initial weights in fit\n",
    "        # delete below if you understand this\n",
    "        # first layer is num_features x num nodes[0]\n",
    "        # middle layers are nodes[i-1] x nodes[i]\n",
    "        # last layers is num_nodes[num_layers-1](represents last layer) x1 (output)\n",
    "        \n",
    "        self.weight_dict = {\n",
    "            0: np.random.rand(num_features, self.nodes[0])\n",
    "        }\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            self.weight_dict[i] = np.random.rand(self.nodes[i-1], self.nodes[i])\n",
    "            \n",
    "        self.weight_dict[self.num_layers] = np.random.rand(self.nodes[self.num_layers-1], 1)\n",
    "\n",
    "    def _forward_propagation(self, X, y):\n",
    "        # performs forward_propagation and stores appropriate values\n",
    "        \n",
    "        sigma_h_x = X\n",
    "        self.stored = {}\n",
    "        self.stored[-1] = Layer(None, sigma_h_x)\n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i] \n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "            self.stored[i] = Layer(h_x, sigma_h_x)\n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[self.num_layers]\n",
    "        self.stored[self.num_layers] = Layer(z_sigma_h_x)\n",
    "        \n",
    "        # average loss\n",
    "        return np.sum(self.loss(z_sigma_h_x, y))/self.batch_size \n",
    "    \n",
    "        \"\"\"\n",
    "        2 hidden layer example DELETE IF YOU UNDERSTAND\n",
    "        dictkey in self.stored: [layer.h_x, layer.sigma_h_x]\n",
    "        -1:[None, x]\n",
    "        0: [h1(x), sigma(h1(x))] #X to h1\n",
    "        1: [h2(sigma(h1(x))), sigma(h2(sigma(h1(x))))] #h1 to h2 [bxn], [bxn]\n",
    "        2: [z(sigma(h2(sigma(h1(x)))))] #h2 to y [nx1]\n",
    "        \"\"\"\n",
    "    def _backward_propagation(self, X,y):\n",
    "        # updates the weights based on the stored values from forward propagation\n",
    "        \n",
    "        num_layers = self.num_layers\n",
    "        expected = self.stored[self.num_layers].h_x #z(sigma(h2(sigma(h1(x)))))\n",
    "        \n",
    "        # dL/dz(z(sigma(h2(sigma(h1(x))))))\n",
    "        J = self.loss(expected, y, derivative = True)\n",
    "        \n",
    "        old_weights = self.weight_dict[num_layers] \n",
    "        self.weight_dict[num_layers] = self.weight_dict[num_layers] - self.lr*(self.stored[num_layers-1].sigma_h_x.T @ J)\n",
    "        # dz/dsigma(sigma(h2(sigma(h1(x)))))\n",
    "        \n",
    "        for i in range(self.num_layers-1,-1,-1):\n",
    "            \n",
    "            J = J * self.sigma(self.stored[i].h_x, derivative = True) # activation layer derivative\n",
    "            \n",
    "            old_weights = self.weight_dict[i] \n",
    "            self.weight_dict[i] = self.weight_dict[i] - self.lr*(self.stored[i-1].sigma_h_x.T @ J) # weight update\n",
    "            \n",
    "            J = J @ old_weights.T  # dense layer derivative\n",
    "            \n",
    "            \n",
    "\n",
    "    def _stochastic_gradient_descent(self, X, y, num_iterations, print_iter):\n",
    "        \n",
    "        avg_err_arr = [np.nan]*num_iterations\n",
    "        \n",
    "        indices = np.random.choice(X.shape[0], self.batch_size, replace=False)\n",
    "        x_batch = X[indices]\n",
    "        y_batch =y[indices]\n",
    "        \n",
    "        avg_err_arr[0] = self._forward_propagation(x_batch,y_batch)\n",
    "\n",
    "        for i in range(1,num_iterations):\n",
    "\n",
    "            self._backward_propagation(x_batch,y_batch)\n",
    "            indices = np.random.choice(X.shape[0], self.batch_size, replace=False)\n",
    "            x_batch = X[indices]\n",
    "            y_batch =y[indices]\n",
    "        \n",
    "            avg_err_arr[i] = self._forward_propagation(x_batch,y_batch)\n",
    "            if print_iter:\n",
    "                sys.stdout.write(\"\\r\" + str(i))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        return avg_err_arr\n",
    "        \n",
    "    \n",
    "    def fit(self, X,y, num_iterations = 1000, print_iter = False):\n",
    "        # X is either a numpy array or a dataframe without the target and intercept\n",
    "        # y is either a pandas series or a 1d (potentially numpy) array\n",
    "        # returns a list of the average loss for each iteration\n",
    "        # if print iter is true, prints the current iteration (will be written on a single line, not multiple)\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.to_numpy()\n",
    "            \n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.to_numpy()\n",
    "        \n",
    "        # add intercept to X\n",
    "        X = np.hstack([np.ones(len(X))[:, np.newaxis], X])  \n",
    "        y = np.array([y]).T # makes it easier to calculate loss\n",
    "        \n",
    "        # generate the random weights for every layer in neural network\n",
    "        self._create_weights(X.shape[1])\n",
    "        \n",
    "        \n",
    "        return self._stochastic_gradient_descent(X,y,num_iterations, print_iter)\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "        # does a single forward propagation without saving any values\n",
    "        # returns a 1xn array of predictions\n",
    "        sigma_h_x = np.hstack([np.ones(len(X))[:, np.newaxis], X])  \n",
    "        \n",
    "        for i in range(0,self.num_layers):\n",
    "            h_x = sigma_h_x @ self.weight_dict[i]\n",
    "            sigma_h_x = self.sigma(h_x) \n",
    "        \n",
    "        z_sigma_h_x = sigma_h_x @ self.weight_dict[self.num_layers]\n",
    "        return z_sigma_h_x.T[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Loss and Activation Function\n",
    "\n",
    "You can use it like so:\n",
    "\n",
    "```\n",
    "test_nn = NeuralNetwork(layers, nodes, activation=ReLU, loss=L2_loss, learning_rate, batch_size)\n",
    "```\n",
    "Just simply enter the name of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and activation function to use in \n",
    "\n",
    "def L2_loss(x,y, derivative = False):\n",
    "    # assumes y is nx1 vector (n rows, 1 col)\n",
    "    # returns an nx1 matrix\n",
    "    if derivative:\n",
    "        return x-y\n",
    "    else:\n",
    "        return 0.5*((x-y)**2)\n",
    "        \n",
    "def ReLU(X, derivative = False):\n",
    "    if derivative:\n",
    "        # faster than np.where\n",
    "        return np.greater(X, 0).astype(int)\n",
    "    return np.maximum(X,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
       "0               3.5                1.4               0.2     5.1\n",
       "1               3.0                1.4               0.2     4.9\n",
       "2               3.2                1.3               0.2     4.7\n",
       "3               3.1                1.5               0.2     4.6\n",
       "4               3.6                1.4               0.2     5.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "data = sklearn.datasets.load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"target\"] = df[\"sepal length (cm)\"]\n",
    "df = df.drop(\"sepal length (cm)\", axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.8, 5.843333333333335, 0.8280661279778629)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.median(),df.target.mean(),df.target.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.3, 7.9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.min(),df.target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 1\n",
    "nodes = 3\n",
    "activation = ReLU\n",
    "loss = L2_loss\n",
    "learning_rate = 0.0001\n",
    "batch_size = 50\n",
    "num_iterations = 1000\n",
    "\n",
    "test_nn = NeuralNetwork(layers, nodes, activation, loss, learning_rate, batch_size)\n",
    "\n",
    "errs = test_nn.fit(df.drop(\"target\",axis=1),df[\"target\"], num_iterations = num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13014751949779435"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_size = 50\n",
    "samp = df.sample(n=samp_size)\n",
    "\n",
    "\n",
    "preds = test_nn.predict(samp.drop(\"target\",axis=1))\n",
    "mse = ((preds -samp.target)**2).sum()/samp_size\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2adada5e048>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x = np.linspace(0, 10, 1000)\n",
    "ax.plot([i for i in range(0,num_iterations)], errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unstable looking result above is normal of stochastic gradient descent, especially if your batch size is small in relation to the overall dataset, as it is an approximation. \n",
    "\n",
    "So as long as the graph shows a general trend downwards, it's fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
